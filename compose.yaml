services:
  # The main entry point for the user
  launcher:
    build: .
    image: ghcr.io/amperecomputingai/ai-playground:0.1
    container_name: demo_launcher
    ports:
      - "7860:7860" # Expose the Gradio UI on port 7860
    volumes:
      # Mount the Docker socket to allow this container to control others
      - /var/run/docker.sock:/var/run/docker.sock

  yolo_demo_service:
    image: ghcr.io/amperecomputingai/ampere-ai-ref-apps:yolov11-0.3.10
    container_name: yolo_demo_service
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:ro
    environment:
      CONFIG_FILE: ${CONFIG_FILE:-cfg/config.yaml}
      GRADIO_SERVER_NAME: ${GRADIO_SERVER_NAME:-0.0.0.0}
      GRADIO_SERVER_PORT: ${GRADIO_SERVER_PORT:-7862}
      HOST_PORT: ${HOST_PORT:-7862}
      INSTANCE_NAME: ${INSTANCE_NAME:-'Ampere'}
      NSTREAMS: ${NSTREAMS:-1}
      NTHREADS: ${NTHREADS:-16}
      VIDEO_SRC: ${VIDEO_SRC:-''}
      WEBCAM0_SRC: ${WEBCAM0_SRC:-0}
      WEBCAM1_SRC: ${WEBCAM1_SRC:-2}
    #network_mode: host
    ports:
      #- "${HOST_PORT}:${GRADIO_SERVER_PORT:-7861}"
      - "7862:7862"

  whisper_demo_service:
    image: ghcr.io/amperecomputingai/ampere-ai-ref-apps:whisper-0.3.10rc1
    container_name: whisper_demo_service
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:ro
    environment:
      CONFIG_FILE: ${CONFIG_FILE:-cfg/config.yaml}
      GRADIO_SERVER_NAME: ${GRADIO_SERVER_NAME:-0.0.0.0}
      GRADIO_SERVER_PORT: ${GRADIO_SERVER_PORT:-7863}
      HOST_PORT: ${HOST_PORT:-7863}
      INSTANCE_NAME: ${INSTANCE_NAME:-'AltraMax'}
      NSTREAMS: ${NSTREAMS:-1}
      NTHREADS: ${NTHREADS:-64}
      AIO_NUM_THREADS: ${AIO_NUM_THREADS:-64}
    ports:
      #- "${HOST_PORT}:${GRADIO_SERVER_PORT:-5001}"
      - "7863:7863"

  ollama_demo_service:
    image: ghcr.io/amperecomputingai/ollama-ampere:1.0.0-ol9
    container_name: ollama_demo_service
    volumes:
      - ollama:/root/.ollama
    #network_mode: host
    ports:
      - "11434:11434"
    environment:
      - "OLLAMA_HOST=0.0.0.0:11434"
    tty: true
    restart: unless-stopped

  llmchat_demo_service:
    image: ghcr.io/open-webui/open-webui:v0.5.20
    container_name: llmchat_demo_service
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama_demo_service
    network_mode: host
    #ports:
    #  - "7863:7863"
    environment:
      - 'PORT=7861'
      - 'ENV=dev'
      - 'VECTOR_DB=chroma'
      - 'OLLAMA_BASE_URL=http://0.0.0.0:11434'
      - 'WEBUI_NAME=Ampere Llama Chat'
      - 'ENABLE_LITELLM=False'
      - 'WEBUI_SECRET_KEY='
      - 'WEBUI_AUTH=False'
    restart: unless-stopped

volumes:
  ollama: {}
  open-webui: {}
